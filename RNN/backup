import torch
import torch.nn.functional as F
from torch import nn
import torch.optim as optim
from torch.autograd import Variable
import random
from torchvision import datasets, transforms
import numpy as np

class Flatten(nn.Module):

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        """forward pass."""
        return x.view(x.size(0), -1)

class CustomModel():
    def __init__(self, build_info, CUDA=True):

        previous_units = 150*8
        self.model = nn.Sequential()
        self.model.add_module('flatten', Flatten())
        if build_info['type']['val'] == 'RNN' :
            for i, layer_info in enumerate(build_info['layers']):
                i = str(i)
                self.model.add_module(
                    'fc_' + i,
                    nn.Linear(previous_units, layer_info['input_size']['val']*8)
                )
                self.model.add_module(
                    'rnn',
                    nn.RNN(layer_info['input_size']['val']*8, layer_info['hidden_size']['val'],\
                       layer_info['num_layers']['val'], layer_info['bias']['val'], True, \
                       layer_info['dropout_rate']['val'], layer_info['bidirectional']['val'])
                )
                previous_units = layer_info['input_size']['val']
        elif build_info['type']['val'] == 'LSTM' :
            for i, layer_info in enumerate(build_info['layers']):
                i = str(i)
                self.model.add_module(
                    'fc_' + i,
                    nn.Linear(previous_units, layer_info['input_size']['val']*8)
                )
                self.model.add_module(
                    'rnn',
                    nn.LSTM(layer_info['input_size']['val']*8, layer_info['hidden_size']['val'],\
                       layer_info['num_layers']['val'], layer_info['bias']['val'], True, \
                       layer_info['dropout_rate']['val'], layer_info['bidirectional']['val'])
                )
                previous_units = layer_info['input_size']['val']
        self.model.add_module(
            'classification_layer',
            nn.Linear(previous_units, 2)
        )
        self.model.add_module('sofmax', nn.LogSoftmax())
        self.model.cpu()

        if build_info['optimizer']['val'] == 'adam':
            optimizer = optim.Adam(self.model.parameters(),
                                   lr=build_info['weight_decay']['val'],
                                   weight_decay=build_info['weight_decay']['val'])

        elif build_info['optimizer']['val'] == 'adadelta':
            optimizer = optim.Adadelta(self.model.parameters(),
                                       lr=build_info['weight_decay']['val'],
                                       weight_decay=build_info['weight_decay']['val'])

        elif build_info['optimizer']['val'] == 'adagrad':
            optimizer = optim.Adagrad(self.model.parameters(),
                                       lr=build_info['weight_decay']['val'],
                                       weight_decay=build_info['weight_decay']['val'])

        elif build_info['optimizer']['val'] == 'rmsprop':
            optimizer = optim.RMSprop(self.model.parameters(),
                                      lr=build_info['weight_decay']['val'],
                                      weight_decay=build_info['weight_decay']['val'])
        else:
            optimizer = optim.SGD(self.model.parameters(),
                                  lr=build_info['weight_decay']['val'],
                                  weight_decay=build_info['weight_decay']['val'],
                                  momentum=0.9)
        self.optimizer = optimizer
        self.cuda = False
        if CUDA:
            self.model.cuda()
            self.cuda = True

    def train(self, train_loader, max_batches=100):
        """Train for 1 epoch."""
        self.model.train()

        batch = 0
        for batch_idx, (dataSet, targetSet) in enumerate(train_loader):
            if self.cuda:
                dataSet, targetSet = dataSet.cuda(), targetSet.cuda()
            _data = Variable(dataSet.view(10, 1, 150, 8)).float()
            _target = Variable(targetSet.view(10, 1, 2)).float()
            self.optimizer.zero_grad()
            output = self.model(_data)
            loss = F.binary_cross_entropy_with_logits(output, _target)
            np_loss = loss.cpu().data.numpy()
            # print((np_loss))
            if np.isnan(np_loss):
                print('stopping training - nan loss')
                return -1
            elif loss.cpu().data.numpy()[0] > 100000:
                print('Qutting, loss too high', np_loss)
                return -1

            loss.backward()
            self.optimizer.step()
            batch += 1
            if batch > max_batches:
                break
        return 1

    def test(self, test_loader, CUDA=False):
        """Evaluate a model."""
        self.model.eval()
        test_loss = 0
        correct = 0
        for data, target in test_loader:
            if self.cuda:
                data, target = data.cuda(), target.cuda()
            data, target = Variable(data, volatile=True).double(), Variable(target).double()
            output = self.model(data)
            test_loss += F.binary_cross_entropy_with_logits(output, target).data[0]
            # get the index of the max log-probability
            pred = output.data.max(1)[1]
            correct += pred.eq(target.data).cpu().sum()

        test_loss /= len(test_loader)
        accuarcy = 100. * correct / len(test_loader.dataset)
        return accuarcy
